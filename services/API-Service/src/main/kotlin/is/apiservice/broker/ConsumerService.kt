package `is`.apiservice.broker

import `is`.apiservice.api.Github
import `is`.apiservice.dto.TaskUserToken
import `is`.apiservice.service.DatasetExportService
import org.slf4j.LoggerFactory
import org.springframework.kafka.annotation.KafkaListener
import org.springframework.stereotype.Service

@Service
class ConsumerService(
    private val github: Github,
    private val datasetExportService: DatasetExportService
) {
    private val log = LoggerFactory.getLogger(javaClass)

    @KafkaListener(topics = ["task_user_token"], groupId = "api-service")
    fun listenTokenUser(taskUserToken: TaskUserToken) {
        try {
            log.info("–ü—Ä–æ—Å–ª—É—à–∏–≤–∞–µ–º task_user_token")
            log.info("\n–ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: \n{}", taskUserToken)

            val token = taskUserToken.token
            val linkIssue = taskUserToken.linkIssue

            if (token != null && linkIssue != null) {
                // –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                val dataset = github.getIssueComments(linkIssue, token)

                if (dataset.isNotEmpty()) {
                    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω issue
                    val minIssue = dataset.minOfOrNull { it.numberIssue } ?: 0
                    val maxIssue = dataset.maxOfOrNull { it.numberIssue } ?: 0
                    val issueRange = "${minIssue}-${maxIssue}"

                    log.info("\n=== –î–ê–¢–ê–°–ï–¢ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ï–í (—Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω –ø–æ Issue) ===")

                    // –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É issue
                    val groupedByIssue = dataset.groupBy { it.numberIssue }

                    groupedByIssue.forEach { (issueNumber, comments) ->
                        log.info("\n--- Issue #{} ({} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤) ---", issueNumber, comments.size)
                        comments.forEach { entry ->
                            log.info(
                                "  <<{}, {}, {}, \"{}\">>",
                                entry.numberIssue,
                                entry.numberCommentInIssue,
                                entry.authorComment,
                                entry.commentText.replace("\n", " ").take(100) +
                                if (entry.commentText.length > 100) "..." else ""
                            )
                        }
                    }

                    log.info("\n=== –ò–¢–û–ì–û: {} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ {} issue ===",
                        dataset.size,
                        groupedByIssue.size
                    )

                    // –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON (–æ–±—ã—á–Ω—ã–π)
                    val flatFilePath = datasetExportService.exportToJson(dataset, issueRange)
                    log.info("\nüìÑ –ü–ª–æ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {}", flatFilePath)

                    // –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON (—Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
                    val groupedFilePath = datasetExportService.exportGroupedByIssue(dataset, issueRange)
                    log.info("üìÑ –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {}\n", groupedFilePath)

                } else {
                    log.warn("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                }

            } else {
                log.warn("–¢–æ–∫–µ–Ω –∏–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ issue –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏: $taskUserToken")
            }

        } catch (e: Exception) {
            log.error("–û—à–∏–±–æ—á–∫–∞ –ø–æ–ª—É—á–∏–ª–∞—Å—å: ${e.message}", e)
        }
    }
}